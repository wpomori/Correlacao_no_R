sqrt(2)
sqrt(4)
sqrt(4)*4
sqrt(4)*4/34567789909767564354323544654335356476587686875655
sqrt(4)*4/34567789909767564354323544654335356476587686875655+1
sqrt(4)*4/34567789909767564354323544654335356476587686875655-1
sqrt(4)*4/34567789909767564354323544654335356476587686875655
sqrt(4)*4/34567789909767564354323544654335356476587686875655 = t
data(varespec)
data(varechem)
library("vegan")
library("vegan")
data(BCI)
S <- specnumber(BCI) # observed number of species
(raremax <- min(rowSums(BCI)))
Srare <- rarefy(BCI, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(BCI, step = 20, sample = raremax, col = "blue", cex = 0.6)
help(rarecurve)
library("vegan")
data(BCI)
S <- specnumber(BCI) # observed number of species
(raremax <- min(rowSums(BCI)))
Srare <- rarefy(BCI, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(BCI, step = 20, sample = raremax, col = "blue", cex = 0.6)
library("vegan")
data(BCI)
S <- specnumber(BCI) # observed number of species
(raremax <- min(rowSums(BCI)))
Srare <- rarefy(BCI, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(BCI, step = 100, sample = raremax, col = "blue", cex = 0.6)
library("vegan")
data(BCI)
S <- specnumber(BCI) # observed number of species
(raremax <- min(rowSums(BCI)))
Srare <- rarefy(BCI, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(BCI, step = 350, sample = raremax, col = "blue", cex = 0.6)
library("vegan")
data(BCI)
S <- specnumber(BCI) # observed number of species
(raremax <- min(rowSums(BCI)))
Srare <- rarefy(BCI, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6)
Srare
rarecurve(Srare, step = 50, sample = raremax, col = "blue", cex = 0.6)
abline(0, 1)
abline(0, 1)
abline(0, 1)
abline(0, 1)
library("vegan")
data(BCI)
S <- specnumber(BCI) # observed number of species
(raremax <- min(rowSums(BCI)))
Srare <- rarefy(BCI, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6)
help("rarecurve")
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6, se)
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6, lty = T)
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6)
rarecurve(BCI, step = 50, sample = raremax, col = "blue", cex = 0.6, grid = F)
library("vegan")
data(BCI)
rarecurve(BCI, step = 20, col = "blue", cex = 0.6, label = FALSE)
beetles <- read.delim ('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/carabid-beetles-boreal-forest.txt', row.names = 1)
rarecurve (t(beetles))
# draw rarefaction curve with confidence intervals
rarecurve.ci <- function (x, step.ci = 1)
{
rar.temp <- apply (com, 1, FUN = function (x) rarefy (x, se = T, sample = 1:sum (x)))
plot.new ()
plot.window (xlim = c(1, max (rowSums (com))), ylim = c(1, max (rowSums (com > 0)) ))
box ()
axis (1)
axis (2)
for (i in seq (1, length (rar.temp)))
{
y <- rar.temp[[i]]
points (y[1,], type = 'l', col = i)
col.ci <- rgb(red = col2rgb (i)[1,], green = col2rgb (i)[2,], blue = col2rgb (i)[3,], alpha = 100, max = 255)
for (x.coord in seq (1, length (y[2,]), by = step.ci))
lines (x = c(x.coord, x.coord), y = c(y[1, x.coord] + 1.95*y[2, x.coord], y[1, x.coord] - 1.95*y[2, x.coord]),
col = col.ci)
}
}
rarecurve.ci (t(beetles))
data(varespec)
## Bray-Curtis distances between samples
dis <- vegdist(varespec)
## First 16 sites grazed, remaining 8 sites ungrazed
groups <- factor(c(rep(1,16), rep(2,8)), labels = c("grazed","ungrazed"))
## Calculate multivariate dispersions
mod <- betadisper(dis, groups)
mod
## Perform test
anova(mod)
## Permutation test for F
permutest(mod, pairwise = TRUE, permutations = 99)
## Tukey's Honest Significant Differences
(mod.HSD <- TukeyHSD(mod))
plot(mod.HSD)
## Plot the groups and distances to centroids on the
## first two PCoA axes
plot(mod)
## with data ellipses instead of hulls
plot(mod, ellipse = TRUE, hull = FALSE) # 1 sd data ellipse
plot(mod, ellipse = TRUE, hull = FALSE, conf = 0.90) # 90% data ellipse
## can also specify which axes to plot, ordering respected
plot(mod, axes = c(3,1), seg.col = "forestgreen", seg.lty = "dashed")
## Draw a boxplot of the distances to centroid for each group
boxplot(mod)
## `scores` and `eigenvals` also work
scrs <- scores(mod)
str(scrs)
head(scores(mod, 1:4, display = "sites"))
# group centroids/medians
scores(mod, 1:4, display = "centroids")
# eigenvalues from the underlying principal coordinates analysis
eigenvals(mod)
## try out bias correction; compare with mod3
(mod3B <- betadisper(dis, groups, type = "median", bias.adjust=TRUE))
anova(mod3B)
permutest(mod3B, permutations = 99)
## should always work for a single group
group <- factor(rep("grazed", NROW(varespec)))
(tmp <- betadisper(dis, group, type = "median"))
(tmp <- betadisper(dis, group, type = "centroid"))
## simulate missing values in 'd' and 'group'
## using spatial medians
groups[c(2,20)] <- NA
dis[c(2, 20)] <- NA
mod2 <- betadisper(dis, groups) ## messages
mod2
permutest(mod2, permutations = 99)
anova(mod2)
plot(mod2)
boxplot(mod2)
plot(TukeyHSD(mod2))
## Using group centroids
mod3 <- betadisper(dis, groups, type = "centroid")
mod3
permutest(mod3, permutations = 99)
anova(mod3)
plot(mod3)
boxplot(mod3)
plot(TukeyHSD(mod3))
data(dune)
mod <- rda(dune, scale = TRUE)
biplot(mod, scaling = "symmetric")
## different type for species and site scores
biplot(mod, scaling = "symmetric", type = c("text", "points"))
data(iris)
df <- iris[c(1, 2, 3, 4)]
autoplot(prcomp(df))
library(ggfortify)
table(aris)
table(iris)
iris
autoplot(prcomp(df))
library(ggfortify)
data(iris)
df <- iris[c(1, 2, 3, 4)]
autoplot(prcomp(df))
autoplot(prcomp(df), data = iris, colour = 'Species')
autoplot(prcomp(df), data = iris, colour = 'Species', label = TRUE, label.size = 3)
autoplot(prcomp(df), data = iris, colour = 'Species', shape = FALSE, label.size = 3)
autoplot(prcomp(df), data = iris, colour = 'Species', loadings = TRUE)
autoplot(prcomp(df), data = iris, colour = 'Species',
loadings = TRUE, loadings.colour = 'blue',
loadings.label = TRUE, loadings.label.size = 3)
autoplot(prcomp(df), scale = 0)
d.factanal <- factanal(state.x77, factors = 3, scores = 'regression')
autoplot(d.factanal, data = state.x77, colour = 'Income')
d.factanal <- factanal(state.x77, factors = 3, scores = 'regression')
autoplot(d.factanal, data = state.x77, colour = 'Income')
d.factanal
autoplot(d.factanal, label = TRUE, label.size = 3,
loadings = TRUE, loadings.label = TRUE, loadings.label.size  = 3)
set.seed(1)
autoplot(kmeans(USArrests, 3), data = USArrests)
autoplot(kmeans(USArrests, 3), data = USArrests, label = TRUE, label.size = 3)
library(cluster)
autoplot(clara(iris[-5], 3))
autoplot(fanny(iris[-5], 3), frame = TRUE)
autoplot(pam(iris[-5], 3), frame = TRUE, frame.type = 'norm')
# Local Fisher Discriminant Analysis (LFDA)
model <- lfda(iris[-5], iris[, 5], r = 3, metric="plain")
autoplot(model, data = iris, frame = TRUE, frame.colour = 'Species')
library(lfda)
library(lfda)
# Local Fisher Discriminant Analysis (LFDA)
model <- lfda(iris[-5], iris[, 5], r = 3, metric="plain")
autoplot(model, data = iris, frame = TRUE, frame.colour = 'Species')
# Semi-supervised Local Fisher Discriminant Analysis (SELF)
model <- self(iris[-5], iris[, 5], beta = 0.1, r = 3, metric="plain")
autoplot(model, data = iris, frame = TRUE, frame.colour = 'Species')
require(ade4) # multivariate analysis
require(ggplot2) # fancy plotting
require(grid) # has the viewport function
require(phyloseq) # GlobalPatterns data
u=seq(2,30,by=2)
v=seq(3,12,by=3)
X1=u%*%t(v) # transpose so the dimensions match in the multiplication
X1
X1=outer(u,v)
X1
# Make a noise matrix to add to X1
Matex <- matrix(rnorm(60,1),nrow=15,ncol=4)
X <- X1+Matex
ggplot(data=data.frame(X), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()
set.seed(0)
n <- 100
p <- 4
Y2 <- outer(rnorm(n), rnorm(p)) + outer(rnorm(n), rnorm(p))
head(Y2)
ggplot(data=data.frame(Y2), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()
# # Example plots with Enterotype Dataset
data(enterotype)
ig <- make_network(enterotype, max.dist=0.3)
library('knitr')
---
title: "Correlaçao no R"
output: html_document
---
library('knitr')
# Correlaçao no R
#
# Correlação é quando queremos ver se uma variável está correlacionada com outra.
# Sempre serão duas variáveis que serão confrontadas (X versus Y).
# Variáveis podem ser de diferentes tipos:
# Quantitativas ===> Contínua (Quando der ideia de peso, altura, renda em reais, comprimento)
#                    medidas, assume vírgula;
#               ===> Discreta (Quando der ideia de contagem: quantidade de carros no estacionamento,
#                    quantidade de filhos, quantidade de erros em uma peça (tudo que for
#                    contagem é variável quantitativa) == não assume vírgula é Discreta.
# Qualitativas  ===> Nominal (Sexo [M/F], Estado civil [Casado, Solteiro, etc])
#               ===> Ordinal (Escolaridade [Fundamental, Médio e Superior])
# X           Y
# Numérica x Numérica
# Numérica x Ordinal
# Numérica x Nominal
# Ordinal x Ordinal
# Ordinal x Nominal
```{r}
# Importando banco de dados com extenção *.TSV
setwd("/home/pine/Documents/CURSOS_EAD/2_Cursos_online_a_parte/2_Udemy/4_correlacao_R")
dados <- read.csv("1_Importando_dados/contabilidade.csv", sep="\t", dec=",", header=TRUE)
dados
```
# Tratamento de dados: selecionando linhas e colunas
#
# Verificando nome das colunas
#
# Coluna Deletar não precisa, pode ser deletada
names(dados)
# Verificando a quantidade de colunas do dataframe "dados"
# 12 colunas
length(names(dados))
# Deletar coluna
dados <- dados[,-(12:27)]
dados
# 11 colunas, última coluna deletada
length(dados)
# Ver as primeiras linhas do objeto dados
head(dados)
names(dados)
# Removendo algumas colunas do dataframe
#
# dados[Linhas, Colunas (-c(2,9,11))]
dados2 <- dados[,-c(2,9,11)]
dados2
# Busco sempre o relacionamento entre duas variáveis (linear e não linear)
# Verificando correlação Graficamente: Numérica x Numérica
# Correlacionar graficamente os dados
library('ggplot2')
library('digest')
names(dados)
# Comportamento não linear
ggplot(data = dados, aes(x = IDADE, y = GASAUDE)) + geom_point()
# Comportamento linear
ggplot(data = dados, aes(x = GASLAZER, y = GASAUDE)) + geom_point()
ggplot(data = dados, aes(x = GASLAZER, y = GASAUDE)) + geom_point() + geom_smooth(method = lm)
# library('digest')
# Como os pontos vermelhos e azuis não estão tendendo a um local (estão dispersos), nenhuma profissão influencia na correlação dentre as duas variáveis
# Aumentar os gastos com Lazer não influencia o aumento com Gasto em saúde, ou seja, as duas variáveis são independentes
ggplot(data = dados, aes(x = GASLAZER, y = GASAUDE, colour = as.factor(PROFI))) + geom_point()
# Quando comparo GASLAZER com GASAUDE e coloro por GASEDU, vejo que as duas primeiras variáveis estão correlacionadas, bem como o GASEDU com elas (todos aumentam)
ggplot(data = dados, aes(x = GASLAZER, y = GASAUDE, colour = GASTEDU)) + geom_point()
# Agora vamos analisar 4 variáveis em um mesmo gráfico. Em geral a relação é linear
# No entanto, o número de filhos não está relacionado com nada, pois o tamanho das bolas não está tendendo para um lado (concentrado em uma direção)
# Até neste ponto, vimos como analisar a relação entre as varíaveis de forma gráfica, antes de fazer os testes estatísticos
ggplot(data = dados, aes(x = GASLAZER, y = GASAUDE, colour = GASTEDU, size=NUNFILHOS)) + geom_point()
# VERIFICANDO A SUPOSIÇÃO DE NORMALIZADADE (INTRODUÇÃO)
#
# Também conhecida como distribuição Gausiana, a normalidade dos dados é um fator fundamental a tomadas de decisões quanto ao uso dos testes estatísticos
# A normalidade é quem decide qual técnica será usada: 1) Paramétrica (dados normais) ou 2) Não paramétrica (dados não normais)
# Para dados de populações pequenas, os dados tendem a não ser normais
# Basicamente, a normalidade está associada a assimetreia na distribuição dos dados. Dados dispersos para uma das extremidades de um histograma, teremos uma assimetria e os dados não são normais
# A assimetria significa que temos a maioria dos dados distantes do centro, logo, não podemos confiar em testes dependentes de média, pois na assimetria isso não estará correto
# O gráfico demonstra uma tendẽncia, os testes comprovão
#
# HISTOGRAMA
# Antes de mais nada, temos de verificar graficamente a tendência dos dados, depois testar estatisticamente
par(mfrow=c(1, 3))
# Assimétrico
hist(dados$GASTEDU)
hist(dados$GASAUDE)
hist(dados$IDADE)
# Uma vez verificado a hipótese de os dados serem normais, agora temos de testar tal hipótese.
# Para isso, podemos usar o QQ-plot. O QQ-plot é um gráfico que compara quantils. Também é conhecido como PP-plot, devido a comparação de percentils. Este gráfico compara os percentils da variável pesquisada contra os percentils de uma variável hipotética.
# Ele compara os dados com uma variável padrão, ou seja, uma variável com desvio padrão 1 e média 0.
# Os quantils da variável padrão é a reta
# Se os pontos da minha amostra estiver quase sobrepondo a reta da variável padrão, isso denota uma distribuição normal
# Reta de normalidade é diferente da reta padrão -> Não confundir
qqnorm(dados$GASTEDU, main = "", xlab = "Quantils teóricos N(0,1)", pch = 20)
qqline(dados$GASTEDU, lty = 2, col = 'red')
qqnorm(dados$GASAUDE, main = "", xlab = "Quantils teóricos N(0,1)", pch = 20)
qqline(dados$GASAUDE, lty = 2, col = 'red')
# Idade não segue uma distribuição normal
qqnorm(dados$IDADE, main = "", xlab = "Quantils teóricos N(0,1)", pch = 20)
qqline(dados$IDADE, lty = 2, col = 'red')
